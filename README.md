# ğŸ›¡ï¸ FinGuard AI - Observable RAG Customer Support Assistant

> Production-ready RAG chatbot for fintech customer support with full observability, powered by Google Gemini AI

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Architecture: Microservices](https://img.shields.io/badge/Architecture-Microservices-orange.svg)](ARCHITECTURE_FINAL.md)

---

## ğŸ“‹ Overview

FinGuard AI is an **open-source, observable RAG** (Retrieval-Augmented Generation) system designed for fintech customer support. It answers customer questions using company policy documents while providing **full transparency** into every step of the process.

**Key Differentiator:** Clean **separated services architecture** with independent AI and Observability services, making it production-ready, testable, and scalable.

---

## âœ¨ Features

### ğŸ” Full Observability
- âœ… Track every step: embedding generation, vector search, LLM generation
- âœ… Latency breakdown (embedding: 45ms, search: 30ms, LLM: 850ms)
- âœ… Real-time metrics dashboard
- âœ… Trace-based monitoring (APM-style)

### ğŸ’° Cost Tracking
- âœ… Token usage tracking (input/output)
- âœ… Real-time cost calculation per query ($0.001-0.003)
- âœ… Session-level cost aggregation
- âœ… Cost breakdown by operation

### ğŸ¯ Quality Assurance
- âœ… Hallucination detection via grounding score
- âœ… Confidence scoring (15% for out-of-scope, 70-95% for grounded)
- âœ… Relevance scoring for retrieved documents
- âœ… Answer quality verification

### ğŸ—ï¸ Clean Architecture
- âœ… **Separated AI Service** - Pure AI/ML operations
- âœ… **Separated Observability Service** - Pure monitoring
- âœ… **RAG Orchestrator** - Clean coordination layer
- âœ… Independent testing and deployment
- âœ… Microservice-ready design

### ğŸš€ Production Ready
- âœ… Docker deployment
- âœ… Error handling & retry logic
- âœ… Session statistics & aggregation
- âœ… Comprehensive logging
- âœ… 23 unit tests (100% passing)

---

## ğŸ—ï¸ Architecture

### High-Level Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Streamlit UI                     â”‚
â”‚  â€¢ Chat interface                       â”‚
â”‚  â€¢ Real-time metrics dashboard          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       RAG Orchestrator                  â”‚
â”‚  Coordinates AI + Observability         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                 â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   AI    â”‚     â”‚ Observability â”‚
    â”‚ Service â”‚     â”‚   Service     â”‚
    â”‚         â”‚     â”‚               â”‚
    â”‚ â€¢ Embed â”‚     â”‚ â€¢ Traces      â”‚
    â”‚ â€¢ Searchâ”‚     â”‚ â€¢ Quality     â”‚
    â”‚ â€¢ Generate    â”‚ â€¢ Metrics     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Read detailed architecture:** [ARCHITECTURE_FINAL.md](ARCHITECTURE.md)

---

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.11+**
- **Google Gemini API Key** ([Get it free here](https://makersuite.google.com/app/apikey))
- **Docker** (optional, for containerized deployment)

### Option 1: Local Setup (Recommended for Development)

1. **Clone the repository**
   ```bash
   git clone https://github.com/yourusername/finguard-observability.git
   cd finguard-observability
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Set up environment variables**
   ```bash
   cp .env.example .env
   # Edit .env and add your GOOGLE_API_KEY
   ```

4. **Run the application**
   ```bash
   streamlit run app/main.py
   ```

5. **Open your browser**
   ```
   http://localhost:8501
   ```

### Option 2: Docker Deployment (Recommended for Production)

1. **Set environment variable**
   ```bash
   export GOOGLE_API_KEY=your_api_key_here
   ```

2. **Build and run**
   ```bash
   docker-compose up -d
   ```

3. **Access the app**
   ```
   http://localhost:8501
   ```

### Option 3: Quick Test (No UI)

```bash
python app/rag_orchestrator.py
```

---

## ğŸ“Š What You Get

### Real-Time Metrics Dashboard

Every query shows:
- **â±ï¸ Latency Breakdown:** Embedding (45ms) + Search (30ms) + LLM (850ms) = Total (925ms)
- **ğŸ’° Cost Tracking:** Input tokens Ã— $0.00001/1K + Output tokens Ã— $0.00003/1K
- **ğŸ¯ Quality Metrics:** Confidence (83%), Grounding score (85%), Hallucination detection
- **ğŸ“„ Source Documents:** Top 3 retrieved docs with relevance scores

### Session Statistics

Aggregated metrics across all queries:
- Total queries processed
- Average latency & P95 latency
- Total cost & average per query
- Hallucination rate
- Success rate

---

## ğŸ§ª Example Queries

Try these questions with FinGuard AI:

### 1. Well-Grounded Query
**Question:** "Why was my payment declined?"

**Expected:**
- âœ… Response based on policy documents
- âœ… High confidence (80-90%)
- âœ… No hallucination detected
- âœ… Latency: ~1.2 seconds
- âœ… Cost: ~$0.002

### 2. Out-of-Scope Query
**Question:** "Who is the prime minister of India?"

**Expected:**
- âœ… Response: "I don't have that information in our policies"
- âœ… Low confidence (15%) - **Correctly uncertain!**
- âœ… No hallucination (properly refuses)

### 3. Multi-Document Query
**Question:** "How long do refunds take?"

**Expected:**
- âœ… Synthesizes info from multiple policy sections
- âœ… High confidence (85-95%)
- âœ… Shows 3 source documents with relevance scores

---

## ğŸ“ Project Structure

```
finguard-observability/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                      # Streamlit UI
â”‚   â”œâ”€â”€ rag_orchestrator.py          # Service coordinator
â”‚   â”‚
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ ai_service.py            # Pure AI operations
â”‚   â”‚   â””â”€â”€ observability_service.py # Pure monitoring
â”‚   â”‚
â”‚   â”œâ”€â”€ embeddings.py                # Gemini embeddings
â”‚   â”œâ”€â”€ vector_store.py              # ChromaDB interface
â”‚   â”œâ”€â”€ llm.py                       # Gemini LLM
â”‚   â””â”€â”€ observability.py             # Quality checks
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ fintech_policies.txt         # Sample policies
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_ai_service.py
â”‚   â”œâ”€â”€ test_observability_service.py
â”‚   â”œâ”€â”€ test_llm.py
â”‚   â””â”€â”€ run_tests.py
â”‚
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ .env.example                     # Environment template
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md                        # This file
â””â”€â”€ ARCHITECTURE_FINAL.md            # Detailed architecture
```

---

## ğŸ”§ Configuration

### Environment Variables (`.env`)

```bash
# Required
GOOGLE_API_KEY=your_gemini_api_key_here

# AI Service Configuration
EMBEDDING_MODEL=models/gemini-embedding-001
LLM_MODEL=models/gemini-2.5-flash
TEMPERATURE=0.1
MAX_TOKENS=500

# RAG Settings
TOP_K_RESULTS=3
CHUNK_SIZE=500
CHUNK_OVERLAP=50

# Vector Store
CHROMA_PERSIST_DIR=./chroma_db
COLLECTION_NAME=fintech_policies
```

### Customization

**Add Your Own Documents:**
1. Place your policy/FAQ documents in `data/` folder
2. Update path in UI or load via interface
3. Documents are automatically chunked and indexed

**Change LLM Model:**
```bash
# In .env
LLM_MODEL=models/gemini-2.5-pro  # More capable (slower, pricier)
LLM_MODEL=models/gemini-2.5-flash # Faster and cheaper (recommended)
```

**Adjust Retrieval:**
```bash
# In .env
TOP_K_RESULTS=5  # More context (better answers, higher cost)
TOP_K_RESULTS=2  # Less context (faster, cheaper)
```

---

## ğŸ§ª Testing

### Run All Tests

```bash
cd finguard-observability
python tests/run_tests.py
```

**Output:**
```
Ran 23 tests in 7.5s

OK âœ“
```

### Test Individual Components

```bash
# Test AI Service
python -m unittest tests.test_ai_service

# Test Observability Service
python -m unittest tests.test_observability_service

# Test RAG Orchestrator
python app/rag_orchestrator.py
```

**Test Coverage:** ~65% (core business logic fully covered)

---

## ğŸ“Š Performance Benchmarks

Typical performance on standard hardware:

| Metric | Value | Notes |
|--------|-------|-------|
| **Total Latency (P50)** | ~900ms | End-to-end |
| **Total Latency (P95)** | ~1200ms | 95th percentile |
| **Embedding Time** | 40-60ms | Gemini API |
| **Vector Search** | 20-40ms | ChromaDB local |
| **LLM Generation** | 700-1000ms | Depends on response length |
| **Cost per Query** | $0.001-0.003 | Average |
| **Throughput** | ~10 queries/min | Single instance |

---

## ğŸ› Troubleshooting

### "GOOGLE_API_KEY not found"
**Solution:** Create `.env` file from `.env.example` and add your API key

### ChromaDB errors
**Solution:** Delete `chroma_db/` folder and re-index documents

### Slow responses
**Solution:**
- Check internet connection
- Try `gemini-2.5-flash` model (faster)
- Reduce `TOP_K_RESULTS`

### High hallucination rate
**Solution:**
- Lower `TEMPERATURE` (try 0.0)
- Increase `TOP_K_RESULTS` for better context
- Improve source documents quality

### Emoji encoding errors (Windows)
**Solution:** The app handles this automatically with UTF-8 encoding

---

## ğŸš€ Deployment

### Docker Deployment

```bash
# Build
docker build -f docker/Dockerfile -t finguard-ai .

# Run
docker run -p 8501:8501 \
  -e GOOGLE_API_KEY=your_key \
  -v $(pwd)/chroma_db:/app/chroma_db \
  finguard-ai
```

### Production Recommendations

- Use managed Kubernetes (GKE, EKS, AKS)
- Deploy AI and Observability services separately
- Use managed vector database (Pinecone, Weaviate)
- Add Redis caching layer
- Integrate with DataDog/Prometheus for monitoring
- Implement rate limiting at API gateway
- Use secrets management (AWS Secrets Manager, etc.)

---

## ğŸ¤ Contributing

Contributions welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Add tests for new features
4. Commit your changes (`git commit -m 'Add amazing feature'`)
5. Push to the branch (`git push origin feature/amazing-feature`)
6. Open a Pull Request

---

## ğŸ“„ License

MIT License - See [LICENSE](LICENSE) file for details

---

## ğŸ™ Acknowledgments

- **Google Gemini** - LLM and embeddings API
- **ChromaDB** - Vector database
- **Streamlit** - UI framework
- **LangChain** - RAG patterns and utilities

---

## ğŸ“ Support

- **Issues:** [GitHub Issues](https://github.com/yourusername/finguard-observability/issues)
- **Discussions:** [GitHub Discussions](https://github.com/yourusername/finguard-observability/discussions)
- **Documentation:** [ARCHITECTURE_FINAL.md](ARCHITECTURE.md)

---

## ğŸ“š Additional Documentation

- **[ARCHITECTURE_FINAL.md](ARCHITECTURE.md)** - Complete system architecture
- **[TEST_SUMMARY.md](TEST_SUMMARY.md)** - Testing documentation
- **[tests/README.md](tests/README.md)** - How to run tests

---

## ğŸ“ Learn More

### What is RAG?
Retrieval-Augmented Generation combines retrieval (finding relevant documents) with generation (LLM creating answers) to provide accurate, grounded responses.

### Why Observability Matters?

**Without observability:**
- âŒ Can't detect hallucinations
- âŒ Can't optimize costs
- âŒ Can't debug failures
- âŒ Can't meet SLAs

**With observability:**
- âœ… Track every step
- âœ… Measure quality
- âœ… Control costs
- âœ… Debug issues
- âœ… Prove compliance

### Why Separated Services?

**Benefits:**
- âœ… Independent testing
- âœ… Easy to swap AI providers
- âœ… Pluggable observability backends
- âœ… Microservice-ready
- âœ… Better maintainability

---

## ğŸ“ˆ Roadmap

### Phase 2: Enhanced Features
- [ ] Multi-model support (OpenAI, Anthropic, Claude)
- [ ] Advanced prompt management and versioning
- [ ] A/B testing framework
- [ ] Fine-tuned embedding models

### Phase 3: Scale & Performance
- [ ] Horizontal scaling support
- [ ] Redis caching layer
- [ ] Async query processing
- [ ] Managed vector database integration

### Phase 4: Enterprise Features
- [ ] Multi-tenancy support
- [ ] Role-based access control (RBAC)
- [ ] Audit logging
- [ ] Compliance reporting
- [ ] Advanced analytics dashboard

---

## â­ Star History

If you find this project useful, please consider giving it a star! â­

---

